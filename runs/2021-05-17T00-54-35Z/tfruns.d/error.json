{
  "message": "TypeError: 'NoneType' object is not iterable",
  "traceback": ["py_call_impl(callable, dots$args, dots$keywords)", "(structure(function (...) \n{\n    dots <- py_resolve_dots(list(...))\n    result <- py_call_impl(callable, dots$args, dots$keywords)\n    if (convert) \n        result <- py_to_r(result)\n    if (is.null(result)) \n        invisible(result)\n    else result\n}, class = c(\"python.builtin.type\", \"python.builtin.object\"), py_object = <environment>))(feature_columns = NULL)", "do.call(layer_class, args)", "create_layer(keras$layers$DenseFeatures, object, list(feature_columns = feature_columns, \n    name = name, trainable = trainable, input_shape = normalize_shape(input_shape), \n    batch_input_shape = normalize_shape(batch_input_shape), batch_size = as_nullable_integer(batch_size), \n    dtype = dtype, weights = weights))", "layer_dense_features(., dense_features(spec_prep))", "create_layer(keras$layers$Dense, object, list(units = as.integer(units), \n    activation = activation, use_bias = use_bias, kernel_initializer = kernel_initializer, \n    bias_initializer = bias_initializer, kernel_regularizer = kernel_regularizer, \n    bias_regularizer = bias_regularizer, activity_regularizer = activity_regularizer, \n    kernel_constraint = kernel_constraint, bias_constraint = bias_constraint, \n    input_shape = normalize_shape(input_shape), batch_input_shape = normalize_shape(batch_input_shape), \n    batch_size = as_nullable_integer(batch_size), dtype = dtype, \n    name = name, trainable = trainable, weights = weights))", "layer_dense(., units = FLAGS$nodes1, activation = \"relu\")", "create_layer(keras$layers$BatchNormalization, object, list(axis = as.integer(axis), \n    momentum = momentum, epsilon = epsilon, center = center, \n    scale = scale, beta_initializer = beta_initializer, gamma_initializer = gamma_initializer, \n    moving_mean_initializer = moving_mean_initializer, moving_variance_initializer = moving_variance_initializer, \n    beta_regularizer = beta_regularizer, gamma_regularizer = gamma_regularizer, \n    beta_constraint = beta_constraint, gamma_constraint = gamma_constraint, \n    renorm = renorm, renorm_clipping = renorm_clipping, renorm_momentum = renorm_momentum, \n    fused = fused, input_shape = normalize_shape(input_shape), \n    batch_input_shape = normalize_shape(batch_input_shape), batch_size = as_nullable_integer(batch_size), \n    dtype = dtype, name = name, trainable = trainable, virtual_batch_size = as_nullable_integer(virtual_batch_size), \n    adjustment = adjustment, weights = weights))", "layer_batch_normalization(.)", "create_layer(keras$layers$Dense, object, list(units = as.integer(units), \n    activation = activation, use_bias = use_bias, kernel_initializer = kernel_initializer, \n    bias_initializer = bias_initializer, kernel_regularizer = kernel_regularizer, \n    bias_regularizer = bias_regularizer, activity_regularizer = activity_regularizer, \n    kernel_constraint = kernel_constraint, bias_constraint = bias_constraint, \n    input_shape = normalize_shape(input_shape), batch_input_shape = normalize_shape(batch_input_shape), \n    batch_size = as_nullable_integer(batch_size), dtype = dtype, \n    name = name, trainable = trainable, weights = weights))", "layer_dense(., units = FLAGS$nodes2, activation = \"relu\", kernel_regularizer = regularizer_l2(l = FLAGS$reg1))", "create_layer(keras$layers$BatchNormalization, object, list(axis = as.integer(axis), \n    momentum = momentum, epsilon = epsilon, center = center, \n    scale = scale, beta_initializer = beta_initializer, gamma_initializer = gamma_initializer, \n    moving_mean_initializer = moving_mean_initializer, moving_variance_initializer = moving_variance_initializer, \n    beta_regularizer = beta_regularizer, gamma_regularizer = gamma_regularizer, \n    beta_constraint = beta_constraint, gamma_constraint = gamma_constraint, \n    renorm = renorm, renorm_clipping = renorm_clipping, renorm_momentum = renorm_momentum, \n    fused = fused, input_shape = normalize_shape(input_shape), \n    batch_input_shape = normalize_shape(batch_input_shape), batch_size = as_nullable_integer(batch_size), \n    dtype = dtype, name = name, trainable = trainable, virtual_batch_size = as_nullable_integer(virtual_batch_size), \n    adjustment = adjustment, weights = weights))", "layer_batch_normalization(.)", "create_layer(keras$layers$Dense, object, list(units = as.integer(units), \n    activation = activation, use_bias = use_bias, kernel_initializer = kernel_initializer, \n    bias_initializer = bias_initializer, kernel_regularizer = kernel_regularizer, \n    bias_regularizer = bias_regularizer, activity_regularizer = activity_regularizer, \n    kernel_constraint = kernel_constraint, bias_constraint = bias_constraint, \n    input_shape = normalize_shape(input_shape), batch_input_shape = normalize_shape(batch_input_shape), \n    batch_size = as_nullable_integer(batch_size), dtype = dtype, \n    name = name, trainable = trainable, weights = weights))", "layer_dense(., units = FLAGS$nodes3, activation = \"relu\", kernel_regularizer = regularizer_l2(l = FLAGS$reg2))", "create_layer(keras$layers$BatchNormalization, object, list(axis = as.integer(axis), \n    momentum = momentum, epsilon = epsilon, center = center, \n    scale = scale, beta_initializer = beta_initializer, gamma_initializer = gamma_initializer, \n    moving_mean_initializer = moving_mean_initializer, moving_variance_initializer = moving_variance_initializer, \n    beta_regularizer = beta_regularizer, gamma_regularizer = gamma_regularizer, \n    beta_constraint = beta_constraint, gamma_constraint = gamma_constraint, \n    renorm = renorm, renorm_clipping = renorm_clipping, renorm_momentum = renorm_momentum, \n    fused = fused, input_shape = normalize_shape(input_shape), \n    batch_input_shape = normalize_shape(batch_input_shape), batch_size = as_nullable_integer(batch_size), \n    dtype = dtype, name = name, trainable = trainable, virtual_batch_size = as_nullable_integer(virtual_batch_size), \n    adjustment = adjustment, weights = weights))", "layer_batch_normalization(.)", "create_layer(keras$layers$Dense, object, list(units = as.integer(units), \n    activation = activation, use_bias = use_bias, kernel_initializer = kernel_initializer, \n    bias_initializer = bias_initializer, kernel_regularizer = kernel_regularizer, \n    bias_regularizer = bias_regularizer, activity_regularizer = activity_regularizer, \n    kernel_constraint = kernel_constraint, bias_constraint = bias_constraint, \n    input_shape = normalize_shape(input_shape), batch_input_shape = normalize_shape(batch_input_shape), \n    batch_size = as_nullable_integer(batch_size), dtype = dtype, \n    name = name, trainable = trainable, weights = weights))", "layer_dense(., units = FLAGS$nodes4, activation = \"relu\")", "create_layer(keras$layers$BatchNormalization, object, list(axis = as.integer(axis), \n    momentum = momentum, epsilon = epsilon, center = center, \n    scale = scale, beta_initializer = beta_initializer, gamma_initializer = gamma_initializer, \n    moving_mean_initializer = moving_mean_initializer, moving_variance_initializer = moving_variance_initializer, \n    beta_regularizer = beta_regularizer, gamma_regularizer = gamma_regularizer, \n    beta_constraint = beta_constraint, gamma_constraint = gamma_constraint, \n    renorm = renorm, renorm_clipping = renorm_clipping, renorm_momentum = renorm_momentum, \n    fused = fused, input_shape = normalize_shape(input_shape), \n    batch_input_shape = normalize_shape(batch_input_shape), batch_size = as_nullable_integer(batch_size), \n    dtype = dtype, name = name, trainable = trainable, virtual_batch_size = as_nullable_integer(virtual_batch_size), \n    adjustment = adjustment, weights = weights))", "layer_batch_normalization(.)", "create_layer(keras$layers$Dense, object, list(units = as.integer(units), \n    activation = activation, use_bias = use_bias, kernel_initializer = kernel_initializer, \n    bias_initializer = bias_initializer, kernel_regularizer = kernel_regularizer, \n    bias_regularizer = bias_regularizer, activity_regularizer = activity_regularizer, \n    kernel_constraint = kernel_constraint, bias_constraint = bias_constraint, \n    input_shape = normalize_shape(input_shape), batch_input_shape = normalize_shape(batch_input_shape), \n    batch_size = as_nullable_integer(batch_size), dtype = dtype, \n    name = name, trainable = trainable, weights = weights))", "layer_dense(., units = 9, activation = \"softmax\")", "compile(., loss = loss_categorical_crossentropy, optimizer = optimizer_adam(lr = FLAGS$lrannea1), \n    metrics = \"accuracy\")", "fit(., dataset_use_spec(train_ds, spec = spec_prep), epochs = 50, \n    callbacks = list(callback_early_stopping(patience = 7), callback_reduce_lr_on_plateau(factor = FLAGS$lrannea2)), \n    validation_data = dataset_use_spec(eval_ds, spec_prep), verbose = 2)", "keras_model_sequential() %>% layer_dense_features(dense_features(spec_prep)) %>% \n    layer_dense(units = FLAGS$nodes1, activation = \"relu\") %>% \n    layer_batch_normalization() %>% layer_dense(units = FLAGS$nodes2, \n    activation = \"relu\", kernel_regularizer = regularizer_l2(l = FLAGS$reg1)) %>% \n    layer_batch_normalization() %>% layer_dense(units = FLAGS$nodes3, \n    activation = \"relu\", kernel_regularizer = regularizer_l2(l = FLAGS$reg2)) %>% \n    layer_batch_normalization() %>% layer_dense(units = FLAGS$nodes4, \n    activation = \"relu\") %>% layer_batch_normalization() %>% \n    layer_dense(units = 9, activation = \"softmax\") %>% compile(loss = loss_categorical_crossentropy, \n    optimizer = optimizer_adam(lr = FLAGS$lrannea1), metrics = \"accuracy\") %>% \n    fit(dataset_use_spec(train_ds, spec = spec_prep), epochs = 50, \n        callbacks = list(callback_early_stopping(patience = 7), \n            callback_reduce_lr_on_plateau(factor = FLAGS$lrannea2)), \n        validation_data = dataset_use_spec(eval_ds, spec_prep), \n        verbose = 2)", "eval(ei, envir)", "eval(ei, envir)", "withVisible(eval(ei, envir))", "tuning_run(\"src/nnet-grid.R\", flags = list(nodes1 = c(32, 64, \n    128), nodes2 = c(32, 64, 128), nodes3 = c(32, 64, 128), nodes4 = c(32, \n    64, 128), reg1 = c(0.25, 0.5, 0.75), reg2 = c(0.25, 0.5, \n    0.75), lrannea1 = c(0.001, 0.01, 0.1), lrannea2 = c(0.1, \n    0.05)), sample = 0.01)"]
}
